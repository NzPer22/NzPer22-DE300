{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1 ####"
      ],
      "metadata": {
        "id": "SgCR5cpySF2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uoED-UpQPsW",
        "outputId": "7ce03939-5e1c-4534-eab9-74e129f70b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  23.3M      0  0:00:01  0:00:01 --:--:-- 23.4M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
      ],
      "metadata": {
        "id": "5KtMdoc5Qh0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QieI2BHlQuKU",
        "outputId": "740a3e28-4445-4612-d32b-8d02d1589a04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agnews = agnews.withColumnRenamed(\"_c0\", \"id\")"
      ],
      "metadata": {
        "id": "EDdBAoDdQ1FQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exploded = agnews.withColumn(\"word\", F.explode(\"filtered\"))\n",
        "word_doc_count = exploded.groupBy(\"id\", \"word\").agg(F.count(\"*\").alias(\"tf\"))\n",
        "doc_total_words = exploded.groupBy(\"id\").agg(F.count(\"*\").alias(\"doc_len\"))\n",
        "tf_df = word_doc_count.join(doc_total_words, on=\"id\").withColumn(\"tf\", F.col(\"tf\") / F.col(\"doc_len\"))"
      ],
      "metadata": {
        "id": "APlhuqjERbQM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = agnews.select(\"id\").distinct().count()\n",
        "docs_with_term = word_doc_count.select(\"word\", \"id\").distinct().groupBy(\"word\").agg(F.count(\"id\").alias(\"doc_freq\"))\n",
        "idf_df = docs_with_term.withColumn(\"idf\", F.log(F.lit(total_docs) / F.col(\"doc_freq\")))"
      ],
      "metadata": {
        "id": "Vaetrt2bRbZT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df = tf_df.join(idf_df, on=\"word\").withColumn(\"tfidf\", F.col(\"tf\") * F.col(\"idf\"))"
      ],
      "metadata": {
        "id": "4-BT_i6QRbcz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowSpec = Window.partitionBy(\"id\").orderBy(F.col(\"tfidf\").desc())\n",
        "ranked = tfidf_df.withColumn(\"rank\", F.row_number().over(windowSpec))\n",
        "top5 = ranked.filter(F.col(\"id\") < 5).orderBy(\"id\", \"rank\")\n",
        "top5.select(\"id\", \"word\", \"tfidf\").show(50, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zIS6gGRgkS",
        "outputId": "e20b4539-2015-495d-8d4c-970dfa745b1a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+-------------------+\n",
            "|id |word         |tfidf              |\n",
            "+---+-------------+-------------------+\n",
            "|0  |cynics       |0.563734318747707  |\n",
            "|0  |wall         |0.5115985326511431 |\n",
            "|0  |claw         |0.499114829314058  |\n",
            "|0  |dwindling    |0.4572386180709258 |\n",
            "|0  |sellers      |0.4468379768438066 |\n",
            "|0  |ultra        |0.4125512394225831 |\n",
            "|0  |seeing       |0.37743394553516213|\n",
            "|0  |band         |0.3643421454792778 |\n",
            "|0  |bears        |0.3372044607529448 |\n",
            "|0  |black        |0.2953171727366614 |\n",
            "|0  |green        |0.2877107940095433 |\n",
            "|0  |short        |0.2773120373951269 |\n",
            "|0  |st           |0.2584728642725166 |\n",
            "|0  |reuters      |0.24754017186645658|\n",
            "|0  |street       |0.24678348986493034|\n",
            "|0  |back         |0.1892216338539946 |\n",
            "|1  |carlyle      |0.7168306746824437 |\n",
            "|1  |occasionally |0.33274321954270536|\n",
            "|1  |timed        |0.324478643568105  |\n",
            "|1  |bets         |0.27861293130724324|\n",
            "|1  |aerospace    |0.2581171817448437 |\n",
            "|1  |reputation   |0.2578098186776328 |\n",
            "|1  |quietly      |0.25188254045524316|\n",
            "|1  |placed       |0.2284965552404658 |\n",
            "|1  |plays        |0.22418048797172685|\n",
            "|1  |controversial|0.20949395177306526|\n",
            "|1  |commercial   |0.2057832028092643 |\n",
            "|1  |looks        |0.1973537176743789 |\n",
            "|1  |private      |0.1929050573011279 |\n",
            "|1  |toward       |0.1898997183872362 |\n",
            "|1  |investment   |0.1890771769001148 |\n",
            "|1  |defense      |0.1751279339938823 |\n",
            "|1  |well         |0.17053284421704767|\n",
            "|1  |making       |0.1698717076460444 |\n",
            "|1  |reuters      |0.1650267812443044 |\n",
            "|1  |part         |0.16022031730914288|\n",
            "|1  |firm         |0.15969712503706046|\n",
            "|1  |industry     |0.15043731768548949|\n",
            "|1  |another      |0.14507889141437585|\n",
            "|1  |market       |0.13394932212703356|\n",
            "|1  |group        |0.12468100563149095|\n",
            "|2  |outlook      |0.4265073217271922 |\n",
            "|2  |doldrums     |0.3770252270329423 |\n",
            "|2  |economy      |0.3721400726458204 |\n",
            "|2  |depth        |0.31343954772064864|\n",
            "|2  |hang         |0.30475018305843793|\n",
            "|2  |cloud        |0.295159450642955  |\n",
            "|2  |soaring      |0.2596334462817101 |\n",
            "|2  |plus         |0.24449073714833106|\n",
            "|2  |worries      |0.23009353850726894|\n",
            "+---+-------------+-------------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2 ####"
      ],
      "metadata": {
        "id": "6ac2dMX8R85B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpPD7N93SDoz",
        "outputId": "2fd43a6b-ce65-4bbe-8f5d-310ea5557897"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   6002      0 --:--:-- --:--:-- --:--:--  6021\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0     99      0 --:--:-- --:--:-- --:--:--    99\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  36.8M      0  0:00:01  0:00:01 --:--:-- 36.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"SVMData\").getOrCreate()\n",
        "\n",
        "# Read CSVs with no header\n",
        "data_svm = spark.read.option(\"header\", \"false\").csv(\"data_for_svm.csv\")\n",
        "w = spark.read.option(\"header\", \"false\").csv(\"w.csv\")\n",
        "bias = spark.read.option(\"header\", \"false\").csv(\"bias.csv\")\n"
      ],
      "metadata": {
        "id": "1TPxGSTvSNRL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss_map(row):\n",
        "    x = np.array([float(row[i]) for i in range(64)], dtype=np.float64)\n",
        "    y = float(row[64])\n",
        "    dot = float(np.dot(w_broadcast.value, x) + b_broadcast.value)\n",
        "    hinge = max(0.0, 1.0 - y * dot)\n",
        "    return hinge\n"
      ],
      "metadata": {
        "id": "Jho7Ib7WS0RP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert w and bias to numpy\n",
        "w_vec = np.array(w.collect(), dtype=np.float64).flatten()\n",
        "b_val = float(bias.collect()[0][0])\n",
        "\n",
        "# Broadcast\n",
        "w_broadcast = spark.sparkContext.broadcast(w_vec)\n",
        "b_broadcast = spark.sparkContext.broadcast(b_val)\n"
      ],
      "metadata": {
        "id": "P_tTh5dWTfN7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hinge_losses = data_svm.rdd.map(svm_loss_map)\n",
        "total_hinge_loss = hinge_losses.reduce(lambda a, b: a + b)\n",
        "mean_hinge_loss = total_hinge_loss / data_svm.count()\n"
      ],
      "metadata": {
        "id": "RVcTMvxvTfCX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_val = 1  # as instructed\n",
        "reg_term = lambda_val * np.dot(w_vec, w_vec)\n",
        "objective = mean_hinge_loss + reg_term"
      ],
      "metadata": {
        "id": "cpCRgeZXTe0L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_SVM(w_df, b_df, data_df, lamb):\n",
        "    w_vec = np.array(w_df.collect(), dtype=np.float64).flatten()\n",
        "    b_val = float(b_df.collect()[0][0])\n",
        "\n",
        "    w_broadcast = spark.sparkContext.broadcast(w_vec)\n",
        "    b_broadcast = spark.sparkContext.broadcast(b_val)\n",
        "\n",
        "    def mapper(row):\n",
        "        x = np.array([float(row[i]) for i in range(64)], dtype=np.float64)\n",
        "        y = float(row[64])\n",
        "        score = float(np.dot(w_broadcast.value, x) + b_broadcast.value)\n",
        "        return max(0.0, 1.0 - y * score)\n",
        "\n",
        "    hinge_losses = data_df.rdd.map(mapper)\n",
        "    total_hinge_loss = hinge_losses.reduce(lambda a, b: a + b)\n",
        "    mean_hinge_loss = total_hinge_loss / data_df.count()\n",
        "    reg_term = lamb * np.dot(w_vec, w_vec)\n",
        "\n",
        "    return mean_hinge_loss + reg_term"
      ],
      "metadata": {
        "id": "ChTlCf9fU6mD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objective_value = loss_SVM(w, bias, data_svm, lamb=1)\n",
        "print(\"SVM Objective Value:\", objective_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kz-quJkVF1R",
        "outputId": "33eff308-7797-457d-8acb-af18767b54fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Objective Value: 1.002940383485752\n"
          ]
        }
      ]
    }
  ]
}